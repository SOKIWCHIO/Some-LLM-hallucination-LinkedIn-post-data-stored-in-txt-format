URL: https://www.linkedin.com/posts/bhaskarjitsarmah_sharing-a-recent-paper-on-ai-hallucinations-activity-7336727768706998272-eq9m
Date: 2025-09-24
Author: Unknown

Sharing a recent paper on AI hallucinations titled "ğ‡ğ€ğ‹ğ¨ğ†ğ„ğ: ğ…ğšğ§ğ­ğšğ¬ğ­ğ¢ğœ ğ‹ğ‹ğŒ ğ‡ğšğ¥ğ¥ğ®ğœğ¢ğ§ğšğ­ğ¢ğ¨ğ§ğ¬ ğšğ§ğ ğ–ğ¡ğğ«ğ ğ­ğ¨ ğ…ğ¢ğ§ğ ğ“ğ¡ğğ¦" introduces HALoGEN, a comprehensive benchmark designed to evaluate and categorize hallucinations in large language models (LLMs). 

Key Insights from this paper - 
ğƒğ¢ğ¯ğğ«ğ¬ğ ğğğ§ğœğ¡ğ¦ğšğ«ğ¤ğ¢ğ§ğ : HALoGEN comprises 10,923 prompts across nine domains, including programming, scientific attribution, and summarization. This diversity ensures a broad assessment of LLM performance. 

ğ€ğ®ğ­ğ¨ğ¦ğšğ­ğğ ğ•ğğ«ğ¢ğŸğ¢ğœğšğ­ğ¢ğ¨ğ§: The framework decomposes LLM outputs into atomic unitsâ€”individual factual statementsâ€”and verifies each against reliable knowledge sources using automated tools.

ğ„ğ±ğ­ğğ§ğ¬ğ¢ğ¯ğ ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§: The study evaluated approximately 150,000 responses from 14 state-of-the-art LLMs, such as GPT-4 and LLaMA. Findings revealed that even top-performing models exhibited hallucination rates up to 86% in certain domains.

ğ„ğ«ğ«ğ¨ğ« ğ‚ğ¥ğšğ¬ğ¬ğ¢ğŸğ¢ğœğšğ­ğ¢ğ¨ğ§:
Type A: Errors stemming from incorrect recollection of training data.
Type B: Errors due to inaccuracies within the training data itself.
Type C: Fabricated information not present in the training data.

ğ‚ğšğ¥ğ¢ğ›ğ«ğšğ­ğ¢ğ¨ğ§ ğ‚ğ¡ğšğ¥ğ¥ğğ§ğ ğğ¬: LLMs often respond to prompts where they should abstain, indicating a need for better calibration mechanisms.

Link is in the first comment