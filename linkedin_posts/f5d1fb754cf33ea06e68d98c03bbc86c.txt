URL: https://www.linkedin.com/posts/whats-ai_ai-machinelearning-llm-activity-7375159358550073344-vj__
Date: 2025-09-24
Author: Unknown

Language models hallucinate not because theyâ€™re â€œbroken,â€ but because our training and evaluation pipelines reward guessing over honesty.

Iâ€™m a bit late to the party on this one (1â€“2 weeks after everyone already shared it), but I finally ğ˜¢ğ˜¤ğ˜µğ˜¶ğ˜¢ğ˜­ğ˜­ğ˜º read ğ—ªğ—µğ˜† ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğ—›ğ—®ğ—¹ğ—¹ğ˜‚ğ—°ğ—¶ğ—»ğ—®ğ˜ğ—² by the OpenAI team. And it sparked a few thoughts worth sharing.

First, to provide context, the paper discusses the hallucination problem and its underlying causes.

Take this example from the paper:

Suppose Model A always admits â€œI donâ€™t knowâ€ when uncertain.

Then, we'd have model B that always guesses a possible answer.

Under most benchmarks (MMLU-Pro, GPQA, SWE-bench, etc.), Model B outperforms Model A, even though it hallucinates.

Why? 

Because binary 0â€“1 grading (coming from answer is good vs. bad) gives no credit to uncertainty. It only credits the actual answer, not if it actually knows it (this is what the table attached shows).

This means even if right 1% of the time, a randomly guessing model will be more successful on most evaluations than one that knows when it does not know.

So todayâ€™s systems are literally trained to bluff.

My takeaway:

We need training signals that reward where idk == idk, true == true, and everything else gets penalized.

But thatâ€™s harder than it sounds because how can we be sure whether a fact was in the pretraining corpus? How to make sure the model indeed does not know? And how do you even design a dataset of â€œhonest IDKsâ€?

And since pretraining is just next-token prediction, not â€œhallucination detection,â€ this problem is deeply baked into the process, even if we try to fix it in the post-training steps.

The paperâ€™s conclusion is pragmatic: we may not fix hallucinations by inventing new â€œhallucination benchmarks.â€ Instead, we need to reform the existing mainstream evaluations to stop punishing uncertainty and abstention.

Until then, weâ€™ll keep optimizing models to be good test-takers, not trustworthy systems.

This is still to reduce hallucination rates and not remove them.

ğŸ‘‰ Would love to know what you think: should we focus more on fixing training data, or on fixing benchmarks?

Or do like Yann LeCun and lose faith in LLMs as a whole ğŸ˜…


è¯é¢˜æ ‡ç­¾
#AI 
è¯é¢˜æ ‡ç­¾
#MachineLearning 
è¯é¢˜æ ‡ç­¾
#LLM 
è¯é¢˜æ ‡ç­¾
#Hallucinations 
è¯é¢˜æ ‡ç­¾
#AIEngineering