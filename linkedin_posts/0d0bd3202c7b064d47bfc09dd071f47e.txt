URL: https://www.linkedin.com/posts/diogoalvesderesende_i-will-make-your-llm-hallucinate-results-activity-7292472376041103361-E81b
Date: 2025-09-24
Author: Unknown

ğŸ”¥Â I will make your LLM hallucinate. Results guaranteed!Â ğŸ”¥

Hereâ€™s a fun fact: Large Language Models love to invent answers when they get confused (we call that â€œhallucinationâ€).

Sometimes they boldly cite articles that donâ€™t exist.

Other times they spin tall tales about historical events that never happened.

But why do LLMs fake answers in the first place?

Because these fancy neural networks guess and predict the next word based on patterns they saw during training.

If they get stuck or have a â€œmemory gap,â€ they might confidently fill it in with made-up info.

In other words, they hallucinate.

Now, imagine you couldÂ promptÂ your LLM so itÂ knowsÂ it should make stuff upâ€”or, better yet, so it harnesses its pre-training â€œmemoriesâ€ in a more grounded, verifiable way.

The paper â€œAccording to . . . : Prompting Language Models Improves Quoting from Pre-Training Dataâ€ shows we can push LLMs to either spit out real quotes or conjure more illusions, just by tweaking the prompt.
Link to paper: https://lnkd.in/dhUtg4i2

Sound wild?

It is.

And itâ€™s awesome.

Today, Iâ€™m releasing a fresh new section in my 6 Days of Prompt Engineering, Generative AI and Data Science course on Udemy:Â Reasoning and Hallucinations for LLMs.

Youâ€™ll see:
1) How toÂ commandÂ your model to reference real documents (â€œAccording to Wikipediaâ€¦â€).
2) How toÂ coaxÂ it into hallucinations to test reliability.
3) Techniques to control and measure its factual grounding.

So if you want your LLM to hallucinate on demandâ€”or if you want it toÂ stopâ€”this new course section is for you.