URL: https://www.linkedin.com/posts/heikohotz_%F0%9D%97%9F%F0%9D%97%9F%F0%9D%97%A0-%F0%9D%97%9B%F0%9D%97%AE%F0%9D%97%B9%F0%9D%97%B9%F0%9D%98%82%F0%9D%97%B0%F0%9D%97%B6%F0%9D%97%BB%F0%9D%97%AE%F0%9D%98%81%F0%9D%97%B6%F0%9D%97%BC%F0%9D%97%BB%F0%9D%98%80-google-activity-7372285384589983744-GZWx
Date: 2025-09-24
Author: Unknown

ğ—Ÿğ—Ÿğ—  ğ—›ğ—®ğ—¹ğ—¹ğ˜‚ğ—°ğ—¶ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€? Google Cloud ğ˜ğ—®ğ—°ğ—¸ğ—¹ğ—²ğ˜€ ğ˜ğ—µğ—² ğ—½ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ğ—µğ—²ğ—®ğ—± ğ—¼ğ—»!

I'm happy to share Gemini Hallcheck, a new open-source toolkit for evaluating model trustworthiness!

A groundbreaking paper by Kalai et al. at OpenAI and Georgia Tech explainsÂ whyÂ hallucinations are a statistically inevitable result of pre-training.
 
Our work provides the first open-source implementation of their core proposal to manage this reality.

Existing benchmarks measure accuracy, but often reward models for confident guessing. This hides the real-world risk of hallucinations and makes it difficult to choose the truly most reliable model for high-stakes tasks.

Building on the theoretical framework from the paper, we've created a practical evaluation suite that moves beyond simple accuracy to measureÂ Behavioural Calibration.

Here are the highlights:
ğŸ¯Â Confidence-Targeted Prompting:Â A new evaluation method that tests if a model can follow risk/reward rules.
âš–ï¸Â Abstention-Aware Scoring:Â Implements the paper's novel penalty scheme to reward honest "I don't know" answers instead of penalizing them.
ğŸ“ˆÂ Trustworthiness Curves:Â Generates a trade-off curve between a model's answer coverage and its conditional accuracy, revealing its true reliability.

Our initial tests show that some models that look best on traditional accuracy benchmarks are not the most behaviourally calibrated. Choosing the right model for your enterprise use case just got a lot clearer ğŸ¤— 

We're open-sourcing our work to help the community build and select more trustworthy AI. Feel free to explore the GitHub repo and run the evaluation on your own models, link to the code in the comments below!