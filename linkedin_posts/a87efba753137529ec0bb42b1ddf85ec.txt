URL: https://www.linkedin.com/posts/ahmedfessi_why-llm-hallucinate-activity-7371788351961600000-k2cw
Date: 2025-09-24
Author: Unknown

I do believe that hallucinations are by far one of the main limitations in LLM. 

Hallucinations are basically plausible falsehoods generated by an LLM (like GPT)

Many scientists (including Yann LeCun from Meta) argue that this specific limitations makes LLM "useless" in a sense and that they can't be the future of AI.

A couple of shared studies on LinkedIn last weeks argued that this is related to some bugs in the data or to some compression failure, but this study from OpenAI seems to me very interesting, because it shows that hallucinations are inherent to LLM (i.e. can't be overcome). 

"Accuracy will never reach 100%"

The paper argues that this is partly related to some open ended questions, but more globally this seems a major flaw that LLM will continue to fix one small step at a time, without reaching a final answer.

Paper attached.